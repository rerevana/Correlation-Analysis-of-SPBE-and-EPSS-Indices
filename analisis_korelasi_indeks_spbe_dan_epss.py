# -*- coding: utf-8 -*-
"""Analisis Korelasi Indeks SPBE dan EPSS - RevanaFS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HjaCR5jp6bf5wvi_9VMEgVBT_iCqZWwB
"""

from google.colab import drive                                        # Untuk menghubungkan Google Drive ke Google Colab.
import pandas as pd                                                   # Untuk manipulasi data (DataFrame).
import seaborn as sns                                                 # Untuk visualisasi data statistik.
import matplotlib.pyplot as plt                                       # Untuk membuat plot dan grafik dasar.
import scipy.stats as stats                                           # Untuk fungsi dan uji statistik.
import statsmodels.api as sm                                          # Untuk model dan analisis statistik.
from sklearn.metrics import mean_squared_error, r2_score              # Untuk metrik evaluasi model regresi (MSE, R^2).
from sklearn.ensemble import RandomForestRegressor                    # Model regresi Random Forest.
import numpy as np                                                    # Untuk operasi numerik dan array.
from xgboost import XGBRegressor                                      # Model regresi XGBoost.
from sklearn.model_selection import RandomizedSearchCV                # Untuk pencarian hyperparameter secara acak.
from sklearn.model_selection import train_test_split, GridSearchCV    # Untuk membagi data (train/test) dan pencarian hyperparameter (grid search).

# Menghubungkan (mount) Google Drive ke direktori '/content/drive' di Colab.
drive.mount('/content/drive')

# Menyimpan path (lokasi) file Excel yang akan dibaca.
file_path = "/content/drive/MyDrive/DataIndeks/type2.xlsx"

# Membaca semua sheet dari file Excel yang ditentukan oleh 'file_path' ke dalam sebuah dictionary bernama 'sheets_dict'.
sheets_dict = pd.read_excel(file_path, sheet_name=None)

# Mencetak daftar nama-nama sheet yang berhasil dibaca dari file Excel.
print("Sheet yang tersedia:", list(sheets_dict.keys()))

# Memeriksa dan melaporkan jumlah data yang hilang (NaN) di setiap sheet.
for sheet_name, df in sheets_dict.items():                # Loop untuk setiap sheet dalam 'sheets_dict'.
    print(f"Missing Values di Sheet: {sheet_name}")       # Tampilkan nama sheet.
    print(df.isnull().sum())                              # Hitung & tampilkan NaN per kolom.
    print(f"Total nilai NaN: {df.isnull().sum().sum()}")  # Hitung & tampilkan total NaN di sheet.
    print("\n")                                           # Baris baru sebagai pemisah.

# Menganalisis secara detail data yang hilang (NaN) pada setiap sheet
for sheet_name, df in sheets_dict.items():      # Melakukan iterasi untuk setiap sheet yang ada dalam 'sheets_dict'.
    print(f"\nProcessing Sheet: {sheet_name}")  # Mencetak nama sheet yang sedang diproses.
    print(f"Jumlah baris (data): {len(df)}")    # Mencetak jumlah total baris (observasi) dalam sheet saat ini.

    # Dictionary 'missing_counts' ini dibuat untuk menyimpan hasil perhitungan jumlah baris yang memenuhi berbagai kondisi kekosongan data (NaN).
    missing_counts = {
        # Menghitung jumlah baris di mana "Digital Government Index 2023" DAN "Digital Government Index 2024" KEDUANYA kosong (NaN).
        "SPBE kosong di 2023 & 2024": df[["Digital Government Index 2023", "Digital Government Index 2024"]].isnull().all(axis=1),

        # Menghitung jumlah baris di mana "Statistical Application Index 2023" DAN "Statistical Application Index 2024" KEDUANYA kosong (NaN).
        "EPSS kosong di 2023 & 2024": df[["Statistical Application Index 2023", "Statistical Application Index 2024"]].isnull().all(axis=1),

        # Menghitung jumlah baris di mana "Digital Government Index 2023" kosong DAN "Digital Government Index 2024" TIDAK kosong.
        "SPBE kosong di 2023 saja": df["Digital Government Index 2023"].isnull() & df["Digital Government Index 2024"].notnull(),

        # Menghitung jumlah baris di mana "Digital Government Index 2024" kosong DAN "Digital Government Index 2023" TIDAK kosong.
        "SPBE kosong di 2024 saja": df["Digital Government Index 2024"].isnull() & df["Digital Government Index 2023"].notnull(),

        # Menghitung jumlah baris di mana "Statistical Application Index 2023" kosong DAN "Statistical Application Index 2024" TIDAK kosong.
        "EPSS kosong di 2023 saja": df["Statistical Application Index 2023"].isnull() & df["Statistical Application Index 2024"].notnull(),

        # Menghitung jumlah baris di mana "Statistical Application Index 2024" kosong DAN "Statistical Application Index 2023" TIDAK kosong.
        "EPSS kosong di 2024 saja": df["Statistical Application Index 2024"].isnull() & df["Statistical Application Index 2023"].notnull()
    }

    # Menampilkan setiap kategori perhitungan data hilang beserta hasilnya dari dictionary 'missing_counts'.
    for key, value in missing_counts.items():  # Melakukan iterasi untuk setiap pasangan kunci (key) dan nilai (value) dalam 'missing_counts'.
        print(f"{key}: {value.sum()} baris")

# Membuat dictionary kosong untuk menyimpan DataFrame yang sudah dibersihkan per sheet.
df_cleaned_sheets = {}

# Membersihkan data pada setiap sheet.
for sheet_name, df in sheets_dict.items():  # Iterasi untuk setiap sheet dan DataFrame di dalamnya.

    # Hapus kolom 'No' dari DataFrame jika kolom tersebut ada.
    if 'No' in df.columns:
        df = df.drop(columns=['No'])  # Menghapus kolom 'No'.

    # Hapus baris jika "Digital Government Index 2023" kosong TAPI "Digital Government Index 2024" terisi.
    # Tanda '~' berarti NOT, jadi baris yang memenuhi kondisi di dalam kurung akan DIHAPUS.
    df = df[~(df["Digital Government Index 2023"].isnull() & df["Digital Government Index 2024"].notnull())]

    # Hapus baris jika salah satu dari "Statistical Application Index" (EPSS) tahun 2023 atau 2024 kosong, sementara yang lainnya terisi.
    # Kondisi: (EPSS 2023 kosong DAN EPSS 2024 terisi) ATAU (EPSS 2023 terisi DAN EPSS 2024 kosong).
    df = df[~((df["Statistical Application Index 2023"].isnull() & df["Statistical Application Index 2024"].notnull()) | \
              (df["Statistical Application Index 2023"].notnull() & df["Statistical Application Index 2024"].isnull()))]

    # Hapus baris jika "Digital Government Index 2023" DAN "Digital Government Index 2024" KEDUANYA kosong.
    df = df[~(df["Digital Government Index 2023"].isnull() & df["Digital Government Index 2024"].isnull())]
    # Hapus baris jika "Statistical Application Index 2023" DAN "Statistical Application Index 2024" KEDUANYA kosong.
    df = df[~(df["Statistical Application Index 2023"].isnull() & df["Statistical Application Index 2024"].isnull())]

    # Isi nilai kosong pada "Digital Government Index 2024" dengan nilai dari "Digital Government Index 2023" pada baris yang sama.
    # Menggunakan .loc untuk memastikan operasi dilakukan pada DataFrame asli.
    df.loc[:, "Digital Government Index 2024"] = df["Digital Government Index 2024"].fillna(df["Digital Government Index 2023"])

    # Menghitung selisih (delta) antara indeks tahun 2024 dan 2023.
    df["Δ Digital Government Index"] = df["Digital Government Index 2024"] - df["Digital Government Index 2023"]
    df["Δ Statistical Application Index"] = df["Statistical Application Index 2024"] - df["Statistical Application Index 2023"]
    # Menghitung persentase perubahan indeks, dibagi 5 (skala maksimum indeks), dikali 100, dan dibulatkan 2 desimal.
    df["% Change in Digital Government Index"] = ((df["Δ Digital Government Index"] / 5) * 100).round(2)
    df["% Change in Statistical Application Index"] = ((df["Δ Statistical Application Index"] / 5) * 100).round(2)

    # Simpan DataFrame yang sudah dibersihkan ke dalam dictionary 'df_cleaned_sheets'.
    df_cleaned_sheets[sheet_name] = df

# Menyimpan semua DataFrame yang telah dibersihkan ke dalam satu file Excel baru.
# Setiap DataFrame akan disimpan sebagai sheet terpisah.
output_path = "/content/drive/MyDrive/DataIndeks/version3_cleaned_data.xlsx"  # Mendefinisikan path untuk file output.
with pd.ExcelWriter(output_path) as writer:                                   # Membuka file Excel untuk ditulis menggunakan ExcelWriter.
    for sheet_name, df in df_cleaned_sheets.items():                          # Iterasi untuk setiap sheet yang sudah dibersihkan.
        df.to_excel(writer, sheet_name=sheet_name, index=False)               # Menulis DataFrame ke sheet tertentu dalam file Excel, tanpa menyertakan indeks DataFrame.
print(f"File hasil cleaning telah disimpan di: {output_path}")                # Mencetak konfirmasi lokasi file yang disimpan.

# Membaca kembali file Excel 'output_path' (yang sebelumnya telah dibersihkan dan disimpan).
# Semua sheet dalam file Excel akan dimuat ke dalam dictionary 'new_sheets_dict'.
new_sheets_dict = pd.read_excel(output_path, sheet_name=None)

# Memeriksa ulang setiap sheet dalam file Excel yang baru dibaca untuk memastikan tidak ada lagi nilai yang hilang (NaN) setelah proses cleaning.
for sheet_name, df in new_sheets_dict.items():        # Iterasi untuk setiap sheet dan DataFrame di dalamnya.
    print(f"Missing Values di Sheet: {sheet_name}")   # Mencetak nama sheet yang sedang diperiksa.

    missing_values = df.isnull().sum()  # Menghitung jumlah nilai NaN untuk setiap kolom dalam sheet saat ini.
    total_missing = missing_values.sum()  # Menghitung total keseluruhan nilai NaN dalam sheet saat ini.

    print(missing_values)  # Menampilkan jumlah NaN per kolom.
    print(f"Total nilai NaN di sheet ini: {total_missing}")  # Menampilkan total keseluruhan NaN di sheet ini.
    print(f"Jumlah baris (data): {len(df)}\n")  # Mencetak jumlah baris (data) dalam sheet saat ini, diikuti baris baru.

    # Blok kondisional: hanya dijalankan jika masih ada nilai NaN yang terdeteksi di sheet.
    if total_missing > 0:
        print("Data yang masih mengandung NaN:")  # Pesan indikasi bahwa baris dengan NaN akan ditampilkan.
        print(df[df.isnull().any(axis=1)])        # Menampilkan semua baris dari DataFrame 'df' yang memiliki setidaknya satu nilai NaN di salah satu kolomnya.

# Fungsi untuk menguji apakah data dalam sebuah kolom terdistribusi normal.
# Metode pengujian (Shapiro-Wilk atau D'Agostino-Pearson) dipilih berdasarkan ukuran sampel.
def test_normality(df, column):
    if len(df) <= 5000:  # Jika jumlah data kurang dari atau sama dengan 5000.
        # Lakukan uji normalitas Shapiro-Wilk, cocok untuk sampel kecil.
        # .dropna() digunakan untuk menghapus nilai NaN sebelum pengujian.
        stat, p_value = stats.shapiro(df[column].dropna())
        method = "Shapiro-Wilk"  # Simpan nama metode yang digunakan.
    else:  # Jika jumlah data lebih dari 5000.
        # Lakukan uji normalitas D'Agostino-Pearson, cocok untuk sampel besar.
        stat, p_value = stats.normaltest(df[column].dropna())
        method = "D’Agostino-Pearson"  # Simpan nama metode yang digunakan.

    return method, p_value  # Kembalikan nama metode dan nilai p (p-value) hasil uji.

# Fungsi untuk menganalisis korelasi antara dua kolom indeks ("% Change in Digital Government Index" dan "% Change in Statistical Application Index")
# dalam DataFrame. Ini melibatkan uji normalitas untuk menentukan metode korelasi yang tepat (Pearson atau Spearman) dan visualisasi hasilnya.
def analyze_correlation(df, sheet_name):
    print(f"\nAnalisis Korelasi - {sheet_name}")  # Cetak judul analisis untuk sheet yang sedang diproses.

    # Uji normalitas untuk kolom "% Change in Digital Government Index".
    method_spbe, p_spbe = test_normality(df, "% Change in Digital Government Index")
    # Uji normalitas untuk kolom "% Change in Statistical Application Index".
    method_epss, p_epss = test_normality(df, "% Change in Statistical Application Index")

    # Cetak hasil uji normalitas untuk kedua indeks. Format p-value menjadi 4 angka desimal.
    print(f"Uji Normalitas ({method_spbe}): % Change in Digital Government Index -> P-value: {p_spbe:.4f}")
    print(f"Uji Normalitas ({method_epss}): % Change in Statistical Application Index -> P-value: {p_epss:.4f}")

    # Tentukan apakah data terdistribusi normal berdasarkan p-value (ambang batas 0.05).
    normal_spbe = p_spbe >= 0.05  # True jika p-value >= 0.05 (data normal).
    normal_epss = p_epss >= 0.05  # True jika p-value >= 0.05 (data normal).

    # Pilih metode korelasi: Pearson jika kedua data normal, Spearman jika salah satu atau keduanya tidak normal.
    if normal_spbe and normal_epss:
        # Hitung korelasi Pearson dan p-value-nya.
        correlation, p_value = stats.pearsonr(df["% Change in Digital Government Index"], df["% Change in Statistical Application Index"])
        method = "Pearson"  # Simpan nama metode korelasi yang digunakan.
    else:
        # Hitung korelasi Spearman dan p-value-nya.
        correlation, p_value = stats.spearmanr(df["% Change in Digital Government Index"], df["% Change in Statistical Application Index"])
        method = "Spearman"  # Simpan nama metode korelasi yang digunakan.

    print(f"Metode Korelasi: {method}")  # Cetak metode korelasi yang digunakan.
    # Cetak nilai koefisien korelasi dan p-value-nya, diformat 4 angka desimal.
    print(f"Korelasi: {correlation:.4f}, P-value: {p_value:.4f}")
    # Cetak kesimpulan signifikansi hubungan berdasarkan p-value.
    print("✅ Hubungan signifikan.\n" if p_value < 0.05 else "❌ Tidak ada hubungan signifikan.\n")

    # Membuat visualisasi scatter plot dengan garis regresi.
    plt.figure(figsize=(6,4))
    sns.regplot(x=df["% Change in Digital Government Index"], y=df["% Change in Statistical Application Index"], ci=None, scatter_kws={'alpha':0.5})
    plt.xlabel("Changes in Digital Government Index")  # Label sumbu x.
    plt.ylabel("Changes in Statistical Application Index")  # Label sumbu y.
    plt.title(f"{method} Correlation of {sheet_name}")  # Judul plot, menyertakan metode dan nama sheet.
    plt.show()  # Tampilkan plot.

    return p_value  # Kembalikan p-value dari uji korelasi.

# List digunakan untuk menyimpan nama-nama sheet yang menunjukkan adanya korelasi signifikan (p-value < 0.05) antara kedua indeks yang dianalisis.
sheets_for_regression = []

# Loop untuk melakukan analisis korelasi pada setiap sheet yang ada dalam 'df_cleaned_sheets'. Jika korelasi pada suatu sheet signifikan,
# nama sheet tersebut akan ditambahkan ke list 'sheets_for_regression'.
for sheet_name, df in df_cleaned_sheets.items():  # Iterasi untuk setiap sheet dan DataFrame yang sudah dibersihkan.
    # Panggil fungsi analyze_correlation untuk melakukan analisis dan mendapatkan p-value korelasi.
    p_value = analyze_correlation(df, sheet_name)
    if p_value < 0.05:  # Jika p-value kurang dari 0.05 (menunjukkan korelasi signifikan).
        # Tambahkan nama sheet ke dalam list 'sheets_for_regression'.
        sheets_for_regression.append(sheet_name)

"""Jika P-value > 0.05, maka gagal menolak hipotesis nol (H₀). Ini berarti tidak cukup bukti statistik untuk menyatakan adanya hubungan yang signifikan pada tingkat signifikansi 5%.

Hipotesis nol (H₀): Tidak ada hubungan yang signifikan antara Δ Indeks SPBE dan Δ Indeks EPSS

Analisis regresi linear dilakukan jika nilai korelasi nya > 0.5 (menunjukkan kekuatan hubungan yang cukup) dan atau p-value nya < 0.05 (menunjukkan hubungan yang signifikan secara statistik).

## Menjalankan analisis regresi untuk sheet yang nilai korelasi nya > 0.5 dan atau p-value nya < 0.05 (Provinsi)
"""

# Fungsi untuk melakukan analisis regresi linear sederhana antara "% Change in Digital Government Index" (variabel independen)
# dan "% Change in Statistical Application Index" (variabel dependen) untuk sheet yang memenuhi kriteria.
def analyze_regression(df, sheet_name):
    print(f"Regression Analysis of {sheet_name}")  # Mencetak judul analisis untuk sheet yang sedang diproses.

    # Menghapus baris yang memiliki nilai NaN pada kolom yang akan digunakan dalam regresi untuk menghindari error saat pemodelan.
    df = df.dropna(subset=["% Change in Digital Government Index", "% Change in Statistical Application Index"])

    # Menentukan variabel independen (X). sm.add_constant menambahkan kolom konstanta (intercept) ke X.
    X = sm.add_constant(df["% Change in Digital Government Index"])
    # Menentukan variabel dependen (y).
    y = df["% Change in Statistical Application Index"]

    # Membuat model regresi linear menggunakan metode Ordinary Least Squares (OLS) dari statsmodels.
    # .fit() digunakan untuk melatih model dengan data X dan y.
    model = sm.OLS(y, X).fit()
    print(model.summary())  # Mencetak ringkasan statistik hasil model regresi (koefisien, R-squared, p-values, dll.).

    # Melakukan prediksi menggunakan model yang sudah dilatih.
    y_pred = model.predict(X)
    # Mencetak Mean Squared Error (MSE) untuk mengevaluasi seberapa dekat prediksi dengan nilai aktual.
    print(f"MSE: {mean_squared_error(y, y_pred):.4f}")
    # Mencetak R-squared (koefisien determinasi) untuk mengukur seberapa baik variabel independen menjelaskan varians variabel dependen.
    print(f"R²: {r2_score(y, y_pred):.4f}")

    # Membuat scatter plot untuk visualisasi data aktual dan garis regresi hasil model.
    plt.figure(figsize=(8, 6))
    sns.regplot(x=df["% Change in Digital Government Index"], y=df["% Change in Statistical Application Index"], scatter_kws={"alpha": 0.5}, line_kws={"color": "red"})
    plt.title(f"Linear Regression of Changes in Digital Government Index & Statistical Application Index ({sheet_name})")  # Judul plot.
    plt.xlabel("% Change in Digital Government Index")  # Label untuk sumbu X.
    plt.ylabel("% Change in Statistical Application Index")  # Label untuk sumbu Y.
    plt.show()  # Menampilkan plot.

# Loop untuk menjalankan analisis regresi linear hanya pada sheet-sheet yang sebelumnya telah diidentifikasi memiliki
# korelasi statistik yang signifikan (disimpan dalam 'sheets_for_regression').
for sheet_name in sheets_for_regression:  # Iterasi melalui setiap nama sheet dalam list 'sheets_for_regression'.
    # Memanggil fungsi analyze_regression untuk melakukan analisis pada DataFrame yang sesuai dengan sheet_name dari 'df_cleaned_sheets'.
    analyze_regression(df_cleaned_sheets[sheet_name], sheet_name)

"""- Model Signifikan: Model regresi Anda secara keseluruhan signifikan secara statistik (Prob(F) = 0.0267), artinya perubahan Indeks SPBE memang memiliki pengaruh yang nyata terhadap perubahan Indeks EPSS, dan ini bukan karena kebetulan.

- Kekuatan Penjelasan Rendah: Meskipun signifikan, model ini hanya mampu menjelaskan sekitar 14,9% (R-squared) dari variasi dalam perubahan Indeks EPSS. Ini berarti sebagian besar (sekitar 85,1%) variasi pada Δ Indeks EPSS dipengaruhi oleh faktor-faktor lain yang tidak ada dalam model ini.

- Pengaruh Variabel Independen: Perubahan Indeks SPBE (Δ Indeks SPBE) terbukti berpengaruh signifikan terhadap perubahan Indeks EPSS (Δ Indeks EPSS), dengan setiap kenaikan 1% pada Δ Indeks SPBE akan meningkatkan Δ Indeks EPSS sekitar 0.4363%.

- Intercept: Ketika Δ Indeks SPBE adalah nol, Δ Indeks EPSS diperkirakan meningkat sebesar 7.1903%.

- Asumsi Model Terpenuhi: Hasil uji Durbin-Watson, normalitas residual (Omnibus & Jarque-Bera), dan Conditional Number menunjukkan bahwa asumsi-asumsi penting dalam regresi linear (seperti tidak ada autokorelasi, residual terdistribusi normal, dan tidak ada multikolinearitas yang serius) telah terpenuhi. Ini meningkatkan validitas kesimpulan yang Anda tarik dari model.

## Kesimpulan
- Terdapat hubungan positif antara % Perubahan Indeks SPBE dan % Perubahan Indeks EPSS, dengan koefisien regresi 0.4363. Artinya, setiap kenaikan 1% dalam Indeks SPBE diperkirakan meningkatkan Indeks EPSS sebesar 0.4363%. Hubungan ini signifikan pada tingkat 5% (p-value = 0.027).

- Namun, model hanya menjelaskan 14,9% variasi dalam Δ Indeks EPSS, yang berarti ada faktor lain yang lebih berpengaruh.

- Ketika tidak ada perubahan dalam Indeks SPBE (Δ Indeks SPBE = 0), Indeks EPSS masih diperkirakan meningkat sekitar 7.19%.

- Scatter plot menunjukkan tren positif, tetapi data cukup tersebar (variabilitas tinggi).
"""

# Fungsiuntuk melakukan tuning hyperparameter model XGBoost Regressoa guna memprediksi "% Change in Statistical Application Index" berdasarkan "% Change in Digital Government Index".
def tune_xgboost(df, sheet_name):
    print(f"\nXGBoost Tuning for {sheet_name}")  # Mencetak nama sheet yang sedang diproses untuk tuning XGBoost.

    # Memastikan tidak ada nilai NaN pada kolom fitur dan target untuk menghindari error saat training.
    df = df.dropna(subset=["% Change in Digital Government Index", "% Change in Statistical Application Index"])

    # Menentukan variabel fitur (X) dan variabel target (y).
    X = df[["% Change in Digital Government Index"]]  # Fitur input (variabel independen).
    y = df["% Change in Statistical Application Index"]  # Target output (variabel dependen).

    # Membagi dataset menjadi data latih (80%) dan data uji (20%).
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Mendefinisikan dictionary 'param_grid' yang berisi daftar hyperparameter dan nilai-nilainya yang akan diuji selama proses Grid Search untuk menemukan kombinasi terbaik.
    param_grid = {
        'n_estimators': [50, 100, 200],  # Jumlah pohon (estimators) dalam model XGBoost.
        'max_depth': [3, 5, 7],  # Kedalaman maksimum setiap pohon.
        'learning_rate': [0.01, 0.1, 0.2],  # Laju pembelajaran (mengontrol seberapa besar penyesuaian bobot pada setiap iterasi).
        'subsample': [0.8, 1.0],  # Fraksi sampel data yang digunakan untuk melatih setiap pohon.
        'colsample_bytree': [0.8, 1.0]  # Fraksi fitur (kolom) yang digunakan untuk melatih setiap pohon.
    }

    # Menginisialisasi model XGBoost Regressor.
    xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

    # Menginisialisasi GridSearchCV untuk mencari kombinasi hyperparameter terbaik.
    # cv=5 berarti menggunakan 5-fold cross-validation.
    # scoring='r2' berarti metrik evaluasi yang digunakan adalah R-squared.
    # n_jobs=-1 menggunakan semua prosesor yang tersedia untuk mempercepat pencarian.
    grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)  # Melatih GridSearchCV dengan data training.

    # Mengambil model dengan hyperparameter terbaik yang ditemukan oleh GridSearchCV.
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)  # Melakukan prediksi pada data test menggunakan model terbaik.

    # Mengevaluasi performa model terbaik pada data test.
    mse = mean_squared_error(y_test, y_pred)  # Menghitung Mean Squared Error.
    r2 = r2_score(y_test, y_pred)  # Menghitung R-squared.

    # Mencetak hyperparameter terbaik dan hasil evaluasi.
    print(f"Best Parameters: {grid_search.best_params_}")  # Menampilkan kombinasi parameter terbaik.
    print(f"MSE: {mse:.4f}")  # Menampilkan nilai MSE.
    print(f"R²: {r2:.4f}")  # Menampilkan nilai R-squared.

    # Membuat scatter plot untuk membandingkan nilai aktual (y_test) dengan nilai prediksi (y_pred).
    plt.figure(figsize=(8,6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.7)
    # Menambahkan garis diagonal y=x sebagai referensi (prediksi sempurna).
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='dashed')
    plt.xlabel("Actual Values of Δ Statistical Application Index")  # Label sumbu x.
    plt.ylabel("Predicted Values of Δ Statistical Application Index")  # Label sumbu y.
    plt.title(f"Predicted vs Actual (XGBoost) for {sheet_name}")  # Judul plot.
    plt.show()  # Menampilkan plot.

# Loopuntuk menjalankan proses tuning hyperparameter XGBoost untuk setiap sheet yang ada dalam dictionary 'new_sheets_dict'.
for sheet_name, df in new_sheets_dict.items():  # Iterasi untuk setiap nama sheet dan DataFrame di dalamnya.
    tune_xgboost(df, sheet_name)  # Memanggil fungsi tune_xgboost untuk sheet saat ini.

"""MSE: Mengukur rata-rata kuadrat dari selisih (error) antara nilai aktual dan nilai prediksi.
Semakin kecil nilai MSE, semakin baik modelnya karena menunjukkan error prediksi yang lebih kecil.

R²: Mengukur proporsi variabilitas dalam variabel dependen (target) yang dapat dijelaskan oleh variabel independen (fitur) dalam model.

1.0 (100%) → Model menjelaskan seluruh variasi data dengan sempurna.

0.0 (0%) → Model tidak lebih baik dari sekadar menggunakan rata-rata nilai target sebagai prediksi.

Nilai negatif → Model lebih buruk daripada hanya menebak rata-rata target.

Dengan analisis menggunakan XGboost, sheet Provinsi yang "lebih baik" dari sheet yang lainnya meskipun akurasi nya terbilang kecil
"""

# Mendefinisikan path (lokasi dan nama file) untuk menyimpan hasil klasifikasi di Google Drive.
output_path = "/content/drive/MyDrive/DataIndeks/version3_classified_performance.xlsx"

# Fungsi untuk mengklasifikasikan kinerja berdasarkan perubahan (delta) pada "Digital Government Index" (Indeks SPBE) dan "Statistical Application Index" (Indeks EPSS).
def classify_performance(df):
    # Mendefinisikan daftar kondisi untuk klasifikasi.
    conditions = [
        (df["Δ Digital Government Index"] > 0) & (df["Δ Statistical Application Index"] > 0),   # Kondisi 1: Kedua indeks meningkat.
        (df["Δ Digital Government Index"] > 0) & (df["Δ Statistical Application Index"] <= 0),  # Kondisi 2: Hanya Indeks SPBE yang meningkat.
        (df["Δ Digital Government Index"] <= 0) & (df["Δ Statistical Application Index"] > 0),  # Kondisi 3: Hanya Indeks EPSS yang meningkat.
        (df["Δ Digital Government Index"] <= 0) & (df["Δ Statistical Application Index"] <= 0), # Kondisi 4: Kedua indeks menurun atau stagnan.
    ]

    # Mendefinisikan label kategori yang sesuai dengan masing-masing kondisi di atas.
    labels = [
        "Kinerja Meningkat",  # Label untuk kondisi 1.
        "Hanya SPBE Naik",    # Label untuk kondisi 2.
        "Hanya EPSS Naik",    # Label untuk kondisi 3.
        "Kinerja Menurun"     # Label untuk kondisi 4.
    ]

    # Membuat kolom baru bernama "Kategori" dalam DataFrame 'df'.
    # Nilai dalam kolom ini ditentukan oleh fungsi np.select berdasarkan 'conditions' dan 'labels'.
    # Jika tidak ada kondisi yang terpenuhi, nilai defaultnya adalah "Tidak Diketahui".
    df["Kategori"] = np.select(conditions, labels, default="Tidak Diketahui")
    return df  # Mengembalikan DataFrame yang sudah memiliki kolom "Kategori".

# Membuat dictionary kosong untuk menyimpan DataFrame dari setiap sheet yang sudah diklasifikasi.
classified_sheets = {}

# Loop untuk memproses setiap sheet dalam 'new_sheets_dict'.
for sheet_name, df in new_sheets_dict.items():  # Iterasi untuk setiap nama sheet dan DataFrame di dalamnya.
    # Menghapus baris yang memiliki nilai NaN pada kolom "% Change in Digital Government Index" atau "% Change in Statistical Application Index".
    df = df.dropna(subset=["% Change in Digital Government Index", "% Change in Statistical Application Index"])
    df_classified = classify_performance(df.copy())  # Melakukan klasifikasi kinerja pada salinan DataFrame.
    classified_sheets[sheet_name] = df_classified  # Menyimpan DataFrame yang sudah diklasifikasi ke dalam dictionary.

# Menyimpan semua DataFrame yang telah diklasifikasi ke dalam satu file Excel baru.
with pd.ExcelWriter(output_path) as writer:           # Membuka file Excel untuk ditulis menggunakan ExcelWriter.
    for sheet_name, df in classified_sheets.items():  # Iterasi untuk setiap sheet yang sudah diklasifikasi.
        # Menulis DataFrame ke sheet tertentu dalam file Excel.
        df.to_excel(writer, sheet_name=sheet_name, index=False)

# Mencetak pesan konfirmasi yang menunjukkan lokasi file hasil klasifikasi telah disimpan.
print(f"File hasil klasifikasi telah disimpan di: {output_path}")